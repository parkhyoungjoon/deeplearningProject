{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "-4khZS7erBTx"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 1. CSV 파일 로드 및 'posts'와 'type' 열 가져오기\n",
        "df = pd.read_csv('/content/MBTI.csv', encoding='utf-8')\n",
        "\n",
        "# P/J만 예측하기 위해 'P'와 'J'를 기준으로 라벨링\n",
        "df['pj_type'] = df['type'].apply(lambda x: 1 if 'N' in x else 0)  # 'E'면 1, 'I'면 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "r_sampled_df = df.groupby('type', group_keys=False).apply(lambda x: x.sample(n=40, random_state=42))\n",
        "r_sampled_df['type'].value_counts()\n",
        "r_sampled_df['pj_type'] = r_sampled_df['type'].apply(lambda x: 1 if 'N' in x else 0)  # 'E'면 1, 'I'면 0"
      ],
      "metadata": {
        "id": "37jjYKKlrFOO"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.DataFrame(columns=['posts','pj_type'] )\n",
        "for index, row in r_sampled_df.iterrows():\n",
        "    # 각 행의 단어를 분리\n",
        "    features = row['posts'].split()\n",
        "    # 해당 단어에 맞는 타겟 값 생성\n",
        "    targets = [row['pj_type']] * len(features)\n",
        "\n",
        "    # 결과 데이터프레임에 추가\n",
        "    dfs = pd.DataFrame({'posts' : features, 'pj_type':targets})\n",
        "    result = pd.concat([result, dfs], ignore_index=True)"
      ],
      "metadata": {
        "id": "fCtc4fxpr-eQ"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트와 라벨 데이터 준비\n",
        "posts = df['posts'].values\n",
        "labels = df['pj_type'].astype('float64').values"
      ],
      "metadata": {
        "id": "vipDQG8bsMXD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. TF-IDF 벡터화\n",
        "tfidf = TfidfVectorizer(max_features=5000)  # 최대 5000개의 단어만 사용\n",
        "X = tfidf.fit_transform(posts)\n",
        "\n",
        "# 3. Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, labels, stratify=labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# PyTorch 텐서로 변환\n",
        "X_train_tensor = torch.FloatTensor(X_train.toarray()).to(DEVICE)\n",
        "X_test_tensor = torch.FloatTensor(X_test.toarray()).to(DEVICE)\n",
        "y_train_tensor = torch.LongTensor(y_train).to(DEVICE)\n",
        "y_test_tensor = torch.LongTensor(y_test).to(DEVICE)\n",
        "x_v_tensor = torch.FloatTensor(X_val.toarray()).to(DEVICE)\n",
        "y_v_tensor = torch.LongTensor(y_val).to(DEVICE)"
      ],
      "metadata": {
        "id": "xmcCyKgAsOGw"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## models 폴더 아래 프로젝트 폴더 아래 모델 파일저장\n",
        "import os\n",
        "\n",
        "# 저장 경로\n",
        "SAVE_PATH = './models/mbti/'\n",
        "# 저장 파일명\n",
        "SAVE_FILE = 'model_train_wbs.pth'\n",
        "# 저장 모델구조 및 파라미터 모두 저장\n",
        "SAVE_MODEL = 'model_all.pth'\n",
        "\n",
        "if not os.path.exists(SAVE_PATH):\n",
        "    os.makedirs(SAVE_PATH)"
      ],
      "metadata": {
        "id": "nBCZUZnft1la"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. 데이터셋 클래스 정의\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.targets[idx]\n",
        "\n",
        "# DataLoader 정의\n",
        "train_dataset = MyDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = MyDataset(X_test_tensor, y_test_tensor)\n",
        "val_dataset = MyDataset(x_v_tensor, y_v_tensor)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 5. MLP 모델 정의 (2층 레이어)\n",
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(MLPModel, self).__init__()\n",
        "\n",
        "        # 은닉층들을 nn.ModuleList로 관리\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "\n",
        "        for hidden_size in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_features, hidden_size))\n",
        "            layers.append(nn.ReLU())  # 활성화 함수로 ReLU 추가\n",
        "            in_features = hidden_size  # 다음 레이어의 입력 크기는 현재 레이어의 출력 크기\n",
        "\n",
        "        # Dropout과 마지막 출력층 추가\n",
        "        layers.append(nn.Dropout(0.3))  # Dropout 추가\n",
        "        layers.append(nn.Linear(in_features, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)  # Sequential로 레이어 묶음\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# 입력 크기는 TF-IDF 벡터 차원인 5000으로 설정\n",
        "input_size = 5000\n",
        "hidden_sizes = [1000, 500, 300, 100, 50]\n",
        "output_size = 1  # 이진 분류\n",
        "\n",
        "model = MLPModel(input_size, hidden_sizes, output_size)\n",
        "model.to(DEVICE)\n",
        "# 6. 손실 함수 및 옵티마이저 정의\n",
        "criterion = nn.BCEWithLogitsLoss()  # 이진 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# 7. 모델 학습\n",
        "epochs = 100\n",
        "patience = 10  # 성능 향상이 없을 때 5번의 에포크 후 학습을 중단\n",
        "\n",
        "best_loss = float('inf')  # 초기값을 매우 큰 값으로 설정\n",
        "trigger_times = 0  # 개선되지 않은 에포크 수\n",
        "\n",
        "LOSS_HISTORY, SCORE_HISTORY = [[],[]],[[],[]]\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    train_pred = []\n",
        "    train_true = []\n",
        "    for features, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features).view(-1)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))\n",
        "        loss = criterion(outputs, targets.float())  # float형으로 변환\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        train_pred.extend(predicted.detach().cpu().numpy())\n",
        "        train_true.extend(targets.detach().cpu().numpy())\n",
        "\n",
        "    train_f1 = f1_score(train_pred, train_true)\n",
        "    avg_loss_train = epoch_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss_train:.4f}, F1: {train_f1:.4f}')\n",
        "\n",
        "    LOSS_HISTORY[0].append(avg_loss_train)\n",
        "    SCORE_HISTORY[0].append(train_f1)\n",
        "\n",
        "    # 8. 교차검증\n",
        "    model.eval()\n",
        "    v_pred = []\n",
        "    v_true = []\n",
        "    v_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for features, targets in val_loader:\n",
        "            outputs = model(features).view(-1)\n",
        "            predicted = torch.round(torch.sigmoid(outputs))  # 시그모이드 함수로 확률 변환\n",
        "            v_pred.extend(predicted.cpu().numpy())\n",
        "            v_true.extend(targets.cpu().numpy())\n",
        "            loss = criterion(outputs, targets.float())\n",
        "            v_loss += loss.item()\n",
        "\n",
        "        val_f1 = f1_score(train_pred, train_true)\n",
        "        avg_loss_val = epoch_loss / len(train_loader)\n",
        "\n",
        "        LOSS_HISTORY[1].append(avg_loss_val)\n",
        "        SCORE_HISTORY[1].append(val_f1)\n",
        "\n",
        "    # Early Stopping 적용\n",
        "    if avg_loss_train < best_loss:\n",
        "        best_loss = avg_loss_train  # 손실이 줄어들면 가장 좋은 손실 업데이트\n",
        "        trigger_times = 0  # 성능이 개선된 경우 카운트를 초기화\n",
        "    else:\n",
        "        trigger_times += 1  # 성능이 개선되지 않은 경우 카운트 증가\n",
        "        print(f\"No improvement in {trigger_times} epochs\")\n",
        "\n",
        "    if trigger_times >= patience:\n",
        "        print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "        break  # 일정 에포크 동안 개선이 없으면 학습 중단\n",
        "\n",
        "    if len(SCORE_HISTORY[1]) == 1:\n",
        "        # 첫번째라서 무조건 모델 파라미터 저장\n",
        "        torch.save(model.state_dict(),SAVE_PATH+SAVE_FILE)\n",
        "        # 모델 전체 저장\n",
        "        torch.save(model,SAVE_PATH+SAVE_MODEL)\n",
        "    else:\n",
        "        if SCORE_HISTORY[1][-1] >= max(SCORE_HISTORY[1]):\n",
        "            # torch.save(model.state_dict(),f'{SAVE_PATH}{SAVE_FILE}_{epoch}_{LOSS_HISTORY[1][-1]}')\n",
        "            torch.save(model.state_dict(),SAVE_PATH+SAVE_FILE)\n",
        "            # 모델 전체 저장\n",
        "            torch.save(model,SAVE_PATH+SAVE_MODEL)\n",
        "\n",
        "# 8. 모델 평가\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for features, targets in test_loader:\n",
        "        outputs = model(features).view(-1)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))  # 시그모이드 함수로 확률 변환\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "\n",
        "# 성능 지표 계산\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7AJEdVSsRMe",
        "outputId": "874eb693-87dc-4fb1-d22e-d86a3f3603c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.1723, F1: 0.9683\n",
            "Epoch [2/100], Loss: 0.0908, F1: 0.9829\n",
            "Epoch [3/100], Loss: 0.0712, F1: 0.9871\n",
            "Epoch [4/100], Loss: 0.0495, F1: 0.9914\n",
            "Epoch [5/100], Loss: 0.0289, F1: 0.9948\n",
            "Epoch [6/100], Loss: 0.0135, F1: 0.9974\n",
            "Epoch [7/100], Loss: 0.0044, F1: 0.9992\n",
            "Epoch [8/100], Loss: 0.0037, F1: 0.9995\n",
            "Epoch [9/100], Loss: 0.0015, F1: 0.9998\n",
            "Epoch [10/100], Loss: 0.0016, F1: 0.9997\n",
            "No improvement in 1 epochs\n",
            "Epoch [11/100], Loss: 0.0008, F1: 0.9999\n",
            "Epoch [12/100], Loss: 0.0016, F1: 0.9998\n",
            "No improvement in 1 epochs\n",
            "Epoch [13/100], Loss: 0.0016, F1: 0.9998\n",
            "No improvement in 2 epochs\n",
            "Epoch [14/100], Loss: 0.0009, F1: 0.9999\n",
            "No improvement in 3 epochs\n",
            "Epoch [15/100], Loss: 0.0009, F1: 0.9999\n",
            "No improvement in 4 epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 텍스트와 라벨 데이터 준비\n",
        "r_posts = result['posts'].values\n",
        "r_labels = result['pj_type'].astype('float64').values\n",
        "r_X = tfidf.fit_transform(r_posts)\n",
        "r_X_test_tensor = torch.FloatTensor(r_X.toarray()).to(DEVICE)\n",
        "r_y_train_tensor = torch.LongTensor(r_labels).to(DEVICE)\n",
        "r_val_dataset = MyDataset(x_v_tensor, y_v_tensor)\n",
        "r_val_loader = DataLoader(dataset=val_dataset, batch_size=32, shuffle=False)\n",
        "model.eval()\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for features, targets in r_val_loader:\n",
        "        outputs = model(features).view(-1)\n",
        "        predicted = torch.round(torch.sigmoid(outputs))  # 시그모이드 함수로 확률 변환\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "\n",
        "# 성능 지표 계산\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(f'F1 Score: {f1:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njf-_vMA1dJJ",
        "outputId": "999a4fe4-3d04-4bdc-c7c2-f5831f2283ad"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9593\n",
            "F1 Score: 0.9778\n"
          ]
        }
      ]
    }
  ]
}